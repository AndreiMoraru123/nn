{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):\n",
    "  X, Y = [], []\n",
    "\n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(3.3544, grad_fn=<NegBackward0>)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "  p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "  t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 27])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs.grad.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 8, 14, 15, 22,  0, 19,  9, 14,  5,  1, 20,  3,  8, 14, 12,  0, 11,  0,\n        26,  9, 25,  0,  1,  1,  7, 18,  9,  3,  5,  9,  0, 18])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yb  # array of correct indices (row 0 will take the 8th column, etc.)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yb.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([3.9176, 3.0297, 3.6506, 3.2596, 4.0721, 3.5237, 3.2332, 3.9260, 3.2319,\n        4.3345, 3.0405, 1.6617, 2.7576, 2.8929, 2.8658, 3.2091, 3.8081, 2.9278,\n        3.6649, 3.5428, 2.8213, 2.9865, 4.4320, 4.0875, 3.5317, 2.9651, 3.1915,\n        3.9038, 2.7102, 3.5855, 3.2697, 3.3062], grad_fn=<NegBackward0>)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-logprobs[range(n), Yb]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "dlogprobs\n",
    "\n",
    "loss = - (a + b + c) / 3\n",
    "loss = -1/3 * a  + -1/3 * b + -1/3 * c\n",
    "dloss/da = -1/3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "dprobs\n",
    "\n",
    "derivative of ln(x) is 1/x\n",
    "\n",
    "so 1/x where x are the probs\n",
    "\n",
    "and then carry the out gradient dlogprobs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "dcounts_sum_inv\n",
    "\n",
    "c = a * b\n",
    "a[3x3] x b[3x1] ---> c[3x1]\n",
    "a11 * b1 + a12 * b1 + a13 * b1\n",
    "a21 * b2 + a22 * b2 + a23 * b2\n",
    "a31 * b3 + a32 * b3 + a33 * b3\n",
    "---> sum across columns (horizontally)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "dcounts_sum\n",
    "\n",
    "division is inverse multiplication so\n",
    "\n",
    "d(1/x) = -1/x^2\n",
    "\n",
    "and then carry the out gradient dcounts_sum_inv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = torch.max(torch.abs(dt - t.grad)).item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([32, 27]), torch.Size([32, 27]), torch.Size([32, 1]))"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_logits.shape, logits.shape, logit_maxes.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "c11 c12 c13 = a11 a12 a13  -   b1\n",
    "c21 c22 c23 = a21 a22 a23  -  b2\n",
    "c31 c32 c33 = a31 a32 a33  -   b3\n",
    "\n",
    "so e.g. c32 = a32 - b3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "27"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape[1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 27])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]).shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x2a42665c8c8>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([32, 64]),\n torch.Size([1, 64]),\n torch.Size([32, 64]),\n torch.Size([1, 64]))"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpreact.shape, bngain.shape, bnraw.shape, bnbias.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([1, 64]), torch.Size([32, 64]))"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnvar.shape, bndiff2.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([32, 30]), torch.Size([32, 3, 10]))"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embcat.shape, emb.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually,\n",
    "# backpropagating through exactly all the variables as\n",
    "# they are defined in the forward pass above, one by one\n",
    "\n",
    "# The derivative of the loss with respect to the logprobs, we took the mean\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1.0 / n\n",
    "\n",
    "# logprobs depends on probs through the log function\n",
    "dprobs = (1.0 / probs) * dlogprobs\n",
    "\n",
    "# probs depends on count and count_sum_inv\n",
    "dcounts_sum_inv = (dprobs * counts).sum(1, keepdim=True)\n",
    "# dcounts = dprobs * counts_sum_inv\n",
    "\n",
    "#counts_sum_inv is just counts_sum inverse\n",
    "dcounts_sum = -1.0 * counts_sum**-2 * dcounts_sum_inv\n",
    "\n",
    "# counts depends on probs (counts * counts_sum_inv) AND counts_sum. Derivative of sum is 1\n",
    "dcounts = dprobs * counts_sum_inv + dcounts_sum * 1\n",
    "\n",
    "# dnorm_logits depends on counts via exponentiation (i.e derivative of exp(x) is exp(x))\n",
    "dnorm_logits = counts * dcounts  # this just propagates the gradient further\n",
    "\n",
    "# logit maxes depends on norm logits via substraction (i.e. derivative of x-y is 1 and -1)\n",
    "dlogit_maxes = -1.0 * dnorm_logits.sum(1, keepdim=True)\n",
    "\n",
    "# logits depends on norm_logits and logit_maxes via addition and subtraction\n",
    "dlogits = dnorm_logits + F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
    "\n",
    "# h depends on logits via a linear transformation (i.e. derivative of x @ W + b is W)\n",
    "dh = dlogits @ W2.T\n",
    "\n",
    "# W2 depends on h via a linear transformation (i.e. derivative of x @ W + b is x)\n",
    "dW2 = h.T @ dlogits\n",
    "\n",
    "# b2 depends on logits via addition (i.e. derivative of x + b is 1)\n",
    "db2 = dlogits.sum(0)\n",
    "\n",
    "# hpreact depends on h via tanh activation (i.e. derivative of tanh(x) is 1 - tanh(x)^2)\n",
    "dhpreact = (1.0 - h**2) * dh\n",
    "\n",
    "# bngain depends on hpreact via a linear transformation (i.e. derivative of a * x + b is a)\n",
    "dbngain = (bnraw * dhpreact).sum(0)  # make sure to sum over the batch dimension\n",
    "\n",
    "# bnbias depends on hpreact via a linear transformation (i.e. derivative of x + b is 1)\n",
    "dbnbias = dhpreact.sum(0)\n",
    "\n",
    "# bnraw depends on hpreact via a linear transformation (i.e. derivative of a * x + b is a)\n",
    "dbnraw = bngain * dhpreact\n",
    "\n",
    "# bnvar_inv depends on bnraw via a multiplication (i.e. derivative of x * y is y)\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0)\n",
    "\n",
    "# bnvar depends on bnvar_inv via negative power -0.5 (i.e. derivative of x^-0.5 is -0.5 * x^-1.5)\n",
    "# but we also have to account for the 1e-5 correction term\n",
    "dbnvar = -0.5 * (bnvar + 1e-5)**-1.5 * dbnvar_inv\n",
    "\n",
    "# bndiff2 deoends in bnvar via multiplication (i.e. derivative of x * y is y)\n",
    "dbndiff2 = ((1/(n-1)) * dbnvar).sum(0)\n",
    "\n",
    "# dbndiff depends on bnraw via a multiplication (i.e. derivative of x * y is y)\n",
    "# AND square root (i.e. derivative of x^0.5 is 0.5 * x^-0.5)\n",
    "dbndiff = (bnvar_inv * dbnraw) + 2.0  * bndiff * dbndiff2\n",
    "\n",
    "# bnmeani depends on bndiff via substraction (i.e. derivative of x - y is 1 and -1)\n",
    "dbnmeani = -1.0 * dbndiff.sum(0)\n",
    "\n",
    "# hprebn depends on bnmeani via multiplication (i.e. derivative of x * y is y)\n",
    "# AND subtraction (i.e. derivative of x - y is 1 and -1)\n",
    "dhprebn = ((1/n) * dbnmeani) + 1.0 * dbndiff\n",
    "\n",
    "# embcat depends on hprebn via a linear transformation (i.e. derivative of x @ W + b is W)\n",
    "dembcat = dhprebn @ W1.T\n",
    "\n",
    "# W1 depends on embcat via a linear transformation (i.e. derivative of x @ W + b is x)\n",
    "dW1 = embcat.T @ dhprebn\n",
    "\n",
    "# b1 depends on hprebn via addition (i.e. derivative of x + b is 1)\n",
    "db1 = dhprebn.sum(0)\n",
    "\n",
    "# embcat depends on emb via a view concatenation (i.e. derivative of x.view(y) is 1)\n",
    "demb = dembcat.view(emb.shape)\n",
    "\n",
    "# emb depends on Xb via a lookup table C (i.e. derivative of C[x] is 1)\n",
    "dC = torch.zeros_like(C)\n",
    "for k in range(Xb.shape[0]):\n",
    "  for j in range(Xb.shape[1]):\n",
    "    dC[Xb[k, j]] += demb[k, j]\n",
    "\n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.35441517829895 diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 6.752088665962219e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "dlogits = F.softmax(logits, dim=1)\n",
    "dlogits[range(n), Yb] -= 1.0\n",
    "dlogits /= n\n",
    "# -----------------\n",
    "\n",
    "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x2a426727248>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 1200x800 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(dlogits.detach(), cmap=\"gray\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: backprop through batchnorm but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
    "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
    "# BatchNorm paper: https://arxiv.org/abs/1502.03167\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / \\\n",
    "               torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# before we had:\n",
    "# dbnraw = bngain * dhpreact\n",
    "# dbndiff = bnvar_inv * dbnraw\n",
    "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "# dbndiff += (2*bndiff) * dbndiff2\n",
    "# dhprebn = dbndiff.clone()\n",
    "# dbnmeani = (-dbndiff).sum(0)\n",
    "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "\n",
    "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
    "# (you'll also need to use some variables from the forward pass up above)\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "dhprebn = bngain * bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1) * bnraw * (dhpreact*bnraw).sum(0))\n",
    "# -----------------\n",
    "\n",
    "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n",
      "      0/ 200000: 3.7969\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4: putting it all together!\n",
    "# Train the MLP neural net with your own backward pass\n",
    "\n",
    "# init\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size # convenience\n",
    "lossi = []\n",
    "\n",
    "# use this context manager for efficiency once your backward pass is written (TODO)\n",
    "#with torch.no_grad():\n",
    "\n",
    "# kick off optimization\n",
    "for i in range(max_steps):\n",
    "\n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  # Linear layer\n",
    "  hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "  # BatchNorm layer\n",
    "  # -------------------------------------------------------------\n",
    "  bnmean = hprebn.mean(0, keepdim=True)\n",
    "  bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "  bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "  bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "  hpreact = bngain * bnraw + bnbias\n",
    "  # -------------------------------------------------------------\n",
    "  # Non-linearity\n",
    "  h = torch.tanh(hpreact) # hidden layer\n",
    "  logits = h @ W2 + b2 # output layer\n",
    "  loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward() # use this for correctness comparisons, delete it later!\n",
    "\n",
    "  # manual backprop! #swole_doge_meme\n",
    "  # -----------------\n",
    "  # YOUR CODE HERE :)\n",
    "  dlogits = F.softmax(logits, dim=1)\n",
    "  dlogits[range(n), Yb] -= 1\n",
    "  dlogits /= n\n",
    "  # 2nd layer (output)\n",
    "  dh = dlogits @ W2.T\n",
    "  dW2 = h.T @ dlogits\n",
    "  db2 = dlogits.sum(0)\n",
    "  # tanh layer\n",
    "  dhpreact = dh * (1 - h**2)\n",
    "  # batchnorm layer\n",
    "  dbngain = (dhpreact * bnraw).sum(0, keepdim=True)\n",
    "  dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "  dhprebn = bngain * bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1) * bnraw * (dhpreact*bnraw).sum(0))\n",
    "  # 1st layer (hidden)\n",
    "  dembcat = dhprebn @ W1.T\n",
    "  dW1 = embcat.T @ dhprebn\n",
    "  db1 = dhprebn.sum(0)\n",
    "  demb = dembcat.view(emb.shape)\n",
    "  dC = torch.zeros_like(C)\n",
    "  for k in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "      dC[Xb[k,j]] += demb[k,j]\n",
    "\n",
    "  grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "  # -----------------\n",
    "\n",
    "  # update\n",
    "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "  for p, grad in zip(parameters, grads):\n",
    "    p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
    "    p.data += -lr * grad # new way of swole doge TODO: enable\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())\n",
    "\n",
    "  if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
    "    break\n",
    "   # pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 10)        | exact: False | approximate: True  | maxdiff: 1.30385160446167e-08\n",
      "(30, 200)       | exact: False | approximate: True  | maxdiff: 9.313225746154785e-09\n",
      "(200,)          | exact: False | approximate: True  | maxdiff: 4.132743924856186e-09\n",
      "(200, 27)       | exact: False | approximate: True  | maxdiff: 1.4901161193847656e-08\n",
      "(27,)           | exact: False | approximate: True  | maxdiff: 4.6566128730773926e-09\n",
      "(1, 200)        | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
      "(1, 200)        | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n"
     ]
    }
   ],
   "source": [
    "# useful for checking your gradients\n",
    "for p,g in zip(parameters, grads):\n",
    "  cmp(str(tuple(p.shape)), g, p)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n",
      "      0/ 200000: 3.8040\n",
      "  10000/ 200000: 2.1809\n",
      "  20000/ 200000: 2.3619\n",
      "  30000/ 200000: 2.4763\n",
      "  40000/ 200000: 1.9443\n",
      "  50000/ 200000: 2.2698\n",
      "  60000/ 200000: 2.4542\n",
      "  70000/ 200000: 2.0235\n",
      "  80000/ 200000: 2.3239\n",
      "  90000/ 200000: 2.1584\n",
      " 100000/ 200000: 1.9188\n",
      " 110000/ 200000: 2.2835\n",
      " 120000/ 200000: 1.9868\n",
      " 130000/ 200000: 2.4621\n",
      " 140000/ 200000: 2.2968\n",
      " 150000/ 200000: 2.1940\n",
      " 160000/ 200000: 1.9667\n",
      " 170000/ 200000: 1.7883\n",
      " 180000/ 200000: 2.0229\n",
      " 190000/ 200000: 1.9305\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4: putting it all together!\n",
    "# Train the MLP neural net with your own backward pass\n",
    "\n",
    "# init\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size # convenience\n",
    "lossi = []\n",
    "\n",
    "# use this context manager for efficiency once your backward pass is written (TODO)\n",
    "#with torch.no_grad():\n",
    "\n",
    "# kick off optimization\n",
    "for i in range(max_steps):\n",
    "\n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  # Linear layer\n",
    "  hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "  # BatchNorm layer\n",
    "  # -------------------------------------------------------------\n",
    "  bnmean = hprebn.mean(0, keepdim=True)\n",
    "  bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "  bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "  bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "  hpreact = bngain * bnraw + bnbias\n",
    "  # -------------------------------------------------------------\n",
    "  # Non-linearity\n",
    "  h = torch.tanh(hpreact) # hidden layer\n",
    "  logits = h @ W2 + b2 # output layer\n",
    "  loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  # loss.backward() # use this for correctness comparisons, delete it later!\n",
    "\n",
    "  # manual backprop! #swole_doge_meme\n",
    "  # -----------------\n",
    "  # YOUR CODE HERE :)\n",
    "  dlogits = F.softmax(logits, dim=1)\n",
    "  dlogits[range(n), Yb] -= 1\n",
    "  dlogits /= n\n",
    "  # 2nd layer (output)\n",
    "  dh = dlogits @ W2.T\n",
    "  dW2 = h.T @ dlogits\n",
    "  db2 = dlogits.sum(0)\n",
    "  # tanh layer\n",
    "  dhpreact = dh * (1 - h**2)\n",
    "  # batchnorm layer\n",
    "  dbngain = (dhpreact * bnraw).sum(0, keepdim=True)\n",
    "  dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "  dhprebn = bngain * bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1) * bnraw * (dhpreact*bnraw).sum(0))\n",
    "  # 1st layer (hidden)\n",
    "  dembcat = dhprebn @ W1.T\n",
    "  dW1 = embcat.T @ dhprebn\n",
    "  db1 = dhprebn.sum(0)\n",
    "  demb = dembcat.view(emb.shape)\n",
    "  dC = torch.zeros_like(C)\n",
    "  for k in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "      dC[Xb[k,j]] += demb[k,j]\n",
    "\n",
    "  grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "  # -----------------\n",
    "\n",
    "  # update\n",
    "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "  for p, grad in zip(parameters, grads):\n",
    "    # p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
    "    p.data += -lr * grad # new way of swole doge TODO: enable\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end of training\n",
    "\n",
    "with torch.no_grad():\n",
    "  # pass the training set through\n",
    "  emb = C[Xtr]\n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  # measure the mean/std over the entire training set\n",
    "  bnmean = hpreact.mean(0, keepdim=True)\n",
    "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.07133150100708\n",
      "val 2.107234477996826\n"
     ]
    }
   ],
   "source": [
    "# evaluate train and val loss\n",
    "\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carmah.\n",
      "amori.\n",
      "kifi.\n",
      "mri.\n",
      "reety.\n",
      "skaelane.\n",
      "rahnen.\n",
      "den.\n",
      "arc.\n",
      "kaeli.\n",
      "nellara.\n",
      "chaiha.\n",
      "kaleigh.\n",
      "ham.\n",
      "join.\n",
      "quint.\n",
      "salin.\n",
      "alvin.\n",
      "quintero.\n",
      "dearyn.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "\n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # forward pass\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,d)\n",
    "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "      hpreact = embcat @ W1 + b1\n",
    "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "      logits = h @ W2 + b2 # (N, vocab_size)\n",
    "      # sample\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "\n",
    "    print(''.join(itos[i] for i in out))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
